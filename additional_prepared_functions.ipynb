{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in CAT_FEATURE_NAMES:\n",
    "    data = pd.concat([data, pd.get_dummies(data[colname], prefix=colname)], axis=1)\n",
    "\n",
    "SELECTED_FEATURE_NAMES = data.columns.drop([TARGET_NAME] + CAT_FEATURE_NAMES).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация данных\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_norm = data.copy()\n",
    "df_norm[NUM_FEATURE_NAMES] = scaler.fit_transform(df_norm[NUM_FEATURE_NAMES])\n",
    "\n",
    "data = df_norm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Балансировка целевой переменной\n",
    "\n",
    "df_for_balancing = pd.concat([X_train, y_train], axis=1)\n",
    "df_balanced = balance_df_by_target(df_for_balancing, TARGET_NAME)\n",
    "    \n",
    "X_train = df_balanced.drop(columns=TARGET_NAME)\n",
    "y_train = df_balanced[TARGET_NAME]\n",
    "\n",
    "# df_balanced[TARGET_NAME].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df_by_target(df, target_name):\n",
    "\n",
    "    target_counts = df[target_name].value_counts()\n",
    "\n",
    "    major_class_name = target_counts.argmax()\n",
    "    minor_class_name = target_counts.argmin()\n",
    "\n",
    "    disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1\n",
    "\n",
    "    for i in range(disbalance_coeff):\n",
    "        sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n",
    "        df = df.append(sample, ignore_index=True)\n",
    "\n",
    "    return df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров\n",
    "\n",
    "params = {'n_estimators':[100, 200, 500, 700, 1000],\n",
    "          'max_depth':[2, 3, 5, 7]}\n",
    "\n",
    "model = catb.CatBoostClassifier(class_weights=[1, 3.5], silent=True, random_state=21, cat_features=cat_features)\n",
    "\n",
    "cv=KFold(n_splits=3, random_state=21, shuffle=True)\n",
    "\n",
    "rs = RandomizedSearchCV(model, params, scoring='f1', cv=cv, n_jobs=-1)\n",
    "rs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n",
    "    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n",
    "\n",
    "    thresholds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for threshold in np.linspace(0.1, 0.9, 9):\n",
    "        thresholds.append(threshold)\n",
    "        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "\n",
    "    scores_table = pd.DataFrame({'f1':f1_scores,\n",
    "                                 'precision':precisions,\n",
    "                                 'recall':recalls,\n",
    "                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n",
    "  \n",
    "    figure = plt.figure(figsize = (15, 5))\n",
    "\n",
    "    plt1 = figure.add_subplot(121)\n",
    "    plt1.plot(thresholds, precisions, label='Precision', linewidth=4)\n",
    "    plt1.plot(thresholds, recalls, label='Recall', linewidth=4)\n",
    "    plt1.plot(thresholds, f1_scores, label='F1', linewidth=4)\n",
    "    plt1.set_ylabel('Scores')\n",
    "    plt1.set_xlabel('Probability threshold')\n",
    "    plt1.set_title('Probabilities threshold calibration')\n",
    "    plt1.legend(bbox_to_anchor=(0.25, 0.25))   \n",
    "    plt1.table(cellText = scores_table.values,\n",
    "               colLabels = scores_table.columns, \n",
    "               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.3, 1, 1])\n",
    "\n",
    "    plt2 = figure.add_subplot(122)\n",
    "    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n",
    "              label='Another class', color='royalblue', alpha=1)\n",
    "    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n",
    "              label='Main class', color='darkcyan', alpha=0.8)\n",
    "    plt2.set_ylabel('Number of examples')\n",
    "    plt2.set_xlabel('Probabilities')\n",
    "    plt2.set_title('Probability histogram')\n",
    "    plt2.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(feature_names, feature_importances, get_top=None):\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "       \n",
    "    plt.figure(figsize = (20, len(feature_importances) * 0.355))\n",
    "    \n",
    "    sns.barplot(feature_importances['importance'], feature_importances['feature'])\n",
    "    \n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Importance of features')\n",
    "    plt.show()\n",
    "    \n",
    "    if get_top is not None:\n",
    "        return feature_importances['feature'][:get_top].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
